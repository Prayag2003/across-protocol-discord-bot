{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df89be8f-2c49-4f4f-9503-2bff0b08a67a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "df89be8f-2c49-4f4f-9503-2bff0b08a67a",
        "outputId": "a2945472-7433-400e-c046-bace7f2bb5b9"
      },
      "outputs": [],
      "source": [
        "%pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain loguru scikit-learn numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e08bb78-7e80-4d95-a124-33b695bf5e6a",
      "metadata": {
        "id": "3e08bb78-7e80-4d95-a124-33b695bf5e6a"
      },
      "source": [
        "`(2) LangSmith`\n",
        "\n",
        "https://docs.smith.langchain.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5258de38-0cc0-4d9d-a5ca-6e750ebe6976",
      "metadata": {
        "id": "5258de38-0cc0-4d9d-a5ca-6e750ebe6976"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feaccdca-1ab0-43b1-82c2-22e9cd27675b",
      "metadata": {
        "id": "feaccdca-1ab0-43b1-82c2-22e9cd27675b"
      },
      "source": [
        "`(3) API Keys`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1cd6453b-2721-491c-b979-1860d58d8cf5",
      "metadata": {
        "id": "1cd6453b-2721-491c-b979-1860d58d8cf5"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1b6e2b-dd76-410d-b870-23e02564a665",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d1b6e2b-dd76-410d-b870-23e02564a665",
        "outputId": "d8252426-ec0e-4d8e-d89b-0bcc8a823950"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import shutil\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from loguru import logger\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Scrape a single page\n",
        "def scrape_page(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        logger.error(f\"Failed to retrieve {url}: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Extract links from a page\n",
        "def extract_links(soup, base_url):\n",
        "    links = []\n",
        "    for a_tag in soup.find_all('a', href=True):\n",
        "        link = a_tag.get('href')\n",
        "        if link:\n",
        "            full_link = urljoin(base_url, link) if not urlparse(link).netloc else link\n",
        "            links.append(full_link)\n",
        "    logger.info(f\"Found {len(links)} links for {base_url}\")\n",
        "    return links\n",
        "\n",
        "# Extract meaningful content from HTML\n",
        "def extract_content(html_content, redundant_data):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    redundant_data_list = [line.strip() for line in redundant_data.split('\\n') if line.strip()]\n",
        "\n",
        "    info = {\n",
        "        \"title\": soup.title.string if soup.title else \"No title\",\n",
        "        \"headers\": [header.get_text() for header in soup.find_all(['h1', 'h2', 'h3'])],\n",
        "        \"paragraphs\": [para.get_text() for para in soup.find_all('p')],\n",
        "        \"lists\": [ul.get_text(separator='\\n').strip() for ul in soup.find_all('ul')],\n",
        "        \"tables\": [],\n",
        "        \"code_blocks\": [],\n",
        "    }\n",
        "\n",
        "    # Remove redundant data\n",
        "    info[\"headers\"] = [header for header in info[\"headers\"] if not any(redundant in header for redundant in redundant_data_list)]\n",
        "    info[\"paragraphs\"] = [para for para in info[\"paragraphs\"] if not any(redundant in para for redundant in redundant_data_list)]\n",
        "    info[\"lists\"] = [ul for ul in info[\"lists\"] if not any(redundant in ul for redundant in redundant_data_list)]\n",
        "\n",
        "    # Extract tables\n",
        "    for table in soup.find_all('table'):\n",
        "        table_data = []\n",
        "        for row in table.find_all('tr'):\n",
        "            row_data = [cell.get_text().strip() for cell in row.find_all(['td', 'th'])]\n",
        "            table_data.append(row_data)\n",
        "        info[\"tables\"].append(table_data)\n",
        "\n",
        "    # Extract code blocks\n",
        "    for code in soup.find_all('code', id=True):\n",
        "        info[\"code_blocks\"].append({\n",
        "            \"id\": code['id'],\n",
        "            \"code\": code.get_text().strip()\n",
        "        })\n",
        "\n",
        "    return info\n",
        "\n",
        "# Recursive crawling and scraping\n",
        "def crawl_and_extract(base_url, depth=1, visited=None, redundant_data=\"\"):\n",
        "    if visited is None:\n",
        "        visited = set()\n",
        "\n",
        "    if depth == 0 or base_url in visited:\n",
        "        return {}\n",
        "\n",
        "    visited.add(base_url)\n",
        "    logger.info(f\"Crawling: {base_url}\")\n",
        "\n",
        "    html_content = scrape_page(base_url)\n",
        "    if not html_content:\n",
        "        return {}\n",
        "\n",
        "    processed_data = {base_url: extract_content(html_content, redundant_data)}\n",
        "\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    links = extract_links(soup, base_url)\n",
        "    for link in links:\n",
        "        processed_data.update(crawl_and_extract(link, depth=depth - 1, visited=visited, redundant_data=redundant_data))\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# Save data to JSON\n",
        "def save_data(data, output_file):\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    temp_file = f\"{output_file}.tmp\"\n",
        "    with open(temp_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "    shutil.move(temp_file, output_file)\n",
        "    logger.info(f\"Data saved to {output_file}\")\n",
        "\n",
        "\n",
        "from langchain.schema import Document\n",
        "# Load data into LangChain's `WebBaseLoader`\n",
        "def load_to_webbase_loader(processed_data):\n",
        "    docs = []\n",
        "    for url, content in processed_data.items():\n",
        "        # Modified this line to handle list of lists in tables\n",
        "        table_text = [\"\\n\".join(str(cell) for cell in row) for row in content[\"tables\"]]\n",
        "        text = \"\\n\\n\".join(\n",
        "            content[\"headers\"] +\n",
        "            content[\"paragraphs\"] +\n",
        "            table_text + # using the modified table text\n",
        "            [code_block[\"code\"] for code_block in content[\"code_blocks\"]]\n",
        "        )\n",
        "        # Create a Document object and append it to the list\n",
        "        docs.append(Document(page_content=text, metadata={\"url\": url}))\n",
        "    return docs\n",
        "\n",
        "# Main pipeline\n",
        "def main_pipeline(scrapers, redundant_data, vectorstore_path):\n",
        "    for scraper in scrapers:\n",
        "        logger.info(f\"Processing {scraper['BASE_URL']}...\")\n",
        "\n",
        "        # Crawl and extract data\n",
        "        processed_data = crawl_and_extract(scraper[\"BASE_URL\"], depth=2, redundant_data=redundant_data)\n",
        "        save_data(processed_data, scraper[\"OUTPUT_FILE\"])\n",
        "\n",
        "        # Load into WebBaseLoader\n",
        "        logger.info(f\"Loading data into WebBaseLoader for {scraper['BASE_URL']}...\")\n",
        "        docs = load_to_webbase_loader(processed_data)\n",
        "\n",
        "        # Split text into chunks\n",
        "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "            chunk_size=300, chunk_overlap=50\n",
        "        )\n",
        "        splits = text_splitter.split_documents(docs)\n",
        "\n",
        "        print(f\"Total splits: {len(splits)}\")\n",
        "\n",
        "        for split in splits:\n",
        "          print(split)\n",
        "        # # Index with Chroma\n",
        "        # logger.info(f\"Indexing data for {scraper['BASE_URL']}...\")\n",
        "        # vectorstore = Chroma.from_documents(splits, embedding=OpenAIEmbeddings(), persist_directory=vectorstore_path)\n",
        "        # # Call persist without any arguments\n",
        "        # vectorstore.persist()\n",
        "        # logger.info(f\"Data indexed and stored at {vectorstore_path}\")\n",
        "\n",
        "redundant_data = \"\"\"👋\n",
        "Introduction\n",
        "Getting Started\n",
        "What is Across?\n",
        "Technical FAQ\n",
        "Migration Guides\n",
        "Migration from V2 to V3\n",
        "Migration to CCTP\n",
        "Migration Guide for Relayers\n",
        "Migration Guide for API Users\n",
        "🔗\n",
        "Use Cases\n",
        "Instant Bridging in your Application\n",
        "Bridge Integration Guide\n",
        "Multi Chain Bridge UI Guide\n",
        "Single Chain Bridge UI Guide\n",
        "Embedded Cross-chain Actions\n",
        "Cross-chain Actions Integration Guide\n",
        "Using the Generic Multicaller Handler Contract\n",
        "Using a Custom Handler Contract\n",
        "Cross-chain Actions UI Guide\n",
        "Settle Cross-chain Intents\n",
        "🧠\n",
        "Concepts\n",
        "What are Cross-chain Intents?\n",
        "Intents Architecture in Across\n",
        "Intent Lifecycle in Across\n",
        "Canonical Asset Maximalism\n",
        "🛠️\n",
        "Reference\n",
        "API Reference\n",
        "SDK Reference\n",
        "Contracts\n",
        "Arbitrum (Chain ID: 42161)\n",
        "Base (Chain ID: 8453)\n",
        "Blast (Chain ID: 81457)\n",
        "Ethereum Mainnet (Chain ID: 1)\n",
        "Linea (Chain ID: 59144)\n",
        "Lisk (Chain ID: 1135)\n",
        "Mode (Chain ID: 34443)\n",
        "Optimism (Chain ID: 10)\n",
        "Polygon (Chain ID: 137)\n",
        "Redstone (Chain ID: 690)\n",
        "Scroll (Chain ID: 534352)\n",
        "World Chain (Chain ID: 480)\n",
        "zkSync (Chain ID: 324)\n",
        "Zora (Chain ID: 7777777)\n",
        "Sepolia Testnet\n",
        "Selected Contract Functions\n",
        "Supported Chains\n",
        "Fees in the System\n",
        "Actors in the System\n",
        "Security Model and Verification\n",
        "Disputing Root Bundles\n",
        "Validating Root Bundles\n",
        "Tracking Events\n",
        "🔁\n",
        "Relayers\n",
        "Running a Relayer\n",
        "Relayer Exclusivity\n",
        "📚\n",
        "Resources\n",
        "Release Notes\n",
        "Developer Support\n",
        "Bug Bounty\n",
        "Audits\n",
        "New Chain Requests\"\"\"\n",
        "\n",
        "scrapers = [\n",
        "    {\"BASE_URL\": \"https://docs.across.to/\", \"OUTPUT_FILE\": \"scraper/v3/processed_data_v3.json\"},\n",
        "    {\"BASE_URL\": \"https://docs.across.to/developer-docs\", \"OUTPUT_FILE\": \"scraper/v2/processed_data_v2.json\"},\n",
        "    {\"BASE_URL\": \"https://docs.across.to/user-docs/\", \"OUTPUT_FILE\": \"scraper/user_docs/processed_data_user_docs.json\"}\n",
        "]\n",
        "\n",
        "main_pipeline(scrapers, redundant_data, vectorstore_path=\"vectorstore/knowledge_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f9b0ed2",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ynoiyjh7GSFx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynoiyjh7GSFx",
        "outputId": "712dee6b-d502-4a47-aa42-8be0fb53f3df"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.schema import Document\n",
        "import json\n",
        "import os\n",
        "\n",
        "def load_processed_json(file_path):\n",
        "    \"\"\"Load processed JSON file.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def convert_to_documents(data):\n",
        "    \"\"\"Convert JSON data into LangChain Document format.\"\"\"\n",
        "    documents = []\n",
        "    for url, content in data.items():\n",
        "        page_content = (\n",
        "            f\"Title: {content.get('title', '')}\\n\"\n",
        "            f\"Headers: {', '.join(content.get('headers', []))}\\n\"\n",
        "            f\"Paragraphs: {' '.join(content.get('paragraphs', []))}\\n\"\n",
        "            f\"Lists: {' '.join(content.get('lists', []))}\\n\"\n",
        "            f\"Tables: {content.get('tables', [])}\\n\"\n",
        "            f\"Code Blocks: {content.get('code_blocks', [])}\"\n",
        "        )\n",
        "        documents.append(Document(page_content=page_content, metadata={\"url\": url}))\n",
        "    return documents\n",
        "\n",
        "def create_embeddings(input_files, output_dir):\n",
        "    \"\"\"Generate embeddings and save them.\"\"\"\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    for file_path in input_files:\n",
        "        logger.info(f\"Processing {file_path} for embeddings...\")\n",
        "        data = load_processed_json(file_path)\n",
        "        documents = convert_to_documents(data)\n",
        "\n",
        "        vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=output_dir)  \n",
        "\n",
        "        # Save vector store to output directory\n",
        "        #output_path = os.path.join(output_dir, os.path.basename(file_path).replace(\".json\", \"_embeddings\"))  # No longer needed\n",
        "        vectorstore.persist() #call persist without any arguments\n",
        "        logger.info(f\"Embeddings saved to {output_dir}\") #log the output directory\n",
        "\n",
        "processed_files = [\n",
        "    \"scraper/v3/processed_data_v3.json\",\n",
        "    \"scraper/v2/processed_data_v2.json\",\n",
        "    \"scraper/user_docs/processed_data_user_docs.json\"\n",
        "]\n",
        "output_directory = \"vector_stores\"\n",
        "\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "create_embeddings(processed_files, output_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "F4kEwLAPDD6s",
      "metadata": {
        "id": "F4kEwLAPDD6s"
      },
      "outputs": [],
      "source": [
        "def generate_prompt_template(context, query, role=\"user\", detail_level=\"standard\", references=[]):\n",
        "    is_code_query = detect_query_type(query)\n",
        "    system_message = get_role_specific_template(role)\n",
        "\n",
        "    output_instruction = {\n",
        "        \"brief\": \"Provide a concise response strictly using KB content.\",\n",
        "        \"standard\": \"Provide a clear, context-specific response strictly based on KB content.\",\n",
        "        \"detailed\": \"Provide an in-depth response strictly within the KB context, with comprehensive details and clarifications.\"\n",
        "    }.get(detail_level, \"Provide a clear, context-specific response strictly based on KB content. Add the source links as well\")\n",
        "\n",
        "    role_instruction = {\n",
        "        \"developer\": \"Craft a precise code solution or explanation based only on KB content, including inline comments and documentation references.\" if is_code_query else \"Offer a technical explanation rooted in KB specifics, avoiding external examples.\",\n",
        "        \"admin\": \"Provide KB-based configuration guidance or protocol references strictly within the KB scope.\" if not is_code_query else \"Offer a KB-referenced code solution adhering to administrative standards.\",\n",
        "        \"user\": \"Explain concepts or provide relevant context strictly using KB content, limiting the response to KB-verified information.\" if not is_code_query else \"Provide a straightforward code solution derived solely from KB content.\"\n",
        "    }.get(role, \"Provide a clear, KB-based response without external references.\")\n",
        "\n",
        "    user_message = f\"\"\"Context from Knowledge Base:\n",
        "{context}\n",
        "\n",
        "User Query:\n",
        "{query}\n",
        "{role_instruction}\n",
        "{output_instruction}\n",
        "\"\"\"\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        "\n",
        "def detect_query_type(query):\n",
        "    \"\"\"Identify if the query requires a code-based response.\"\"\"\n",
        "    code_indicators = [\n",
        "        'code', 'function', 'implement', 'write', 'program',\n",
        "        'syntax', 'debug', 'error', 'example', 'script',\n",
        "        'development', 'api', 'integration'\n",
        "    ]\n",
        "    return any(indicator in query.lower() for indicator in code_indicators)\n",
        "\n",
        "def get_role_specific_template(role):\n",
        "    \"\"\"Define a role-specific system message with strict KB adherence and minimal external assumptions.\"\"\"\n",
        "    if role == \"developer\":\n",
        "        return \"\"\"You are a highly skilled developer assistant with expertise in Across protocol. Follow these guidelines:\n",
        "        - Use the knowledge base strictly as the primary source of truth.\n",
        "        - Avoid assuming knowledge outside of the KB.\n",
        "        - Provide production-ready code adhering to best practices, with inline comments and structured documentation.\n",
        "        - Use KB content to address context cues and avoid adding creative assumptions outside the KB.\"\"\"\n",
        "\n",
        "    elif role == \"admin\":\n",
        "        return \"\"\"You are an administrative assistant specializing in Across protocol operations and configurations. Your approach should focus on:\n",
        "        - Ensuring all responses derive strictly from KB content.\n",
        "        - Providing configuration details, protocol rules, and policy insights only from verified KB information.\n",
        "        - Minimizing external explanations, focusing on KB-specific instructions and guidelines only.\"\"\"\n",
        "\n",
        "    else:  # Default to general user\n",
        "        return \"\"\"You are a knowledgeable Across protocol assistant. Respond to user queries as follows:\n",
        "        - Use the KB as your sole source of truth for information.\n",
        "        - Break down technical concepts into digestible explanations without adding creative assumptions.\n",
        "        - Refer only to KB content, ensuring accuracy and adherence to the source material.\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-l9s1DHQ-lyP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l9s1DHQ-lyP",
        "outputId": "4ee89a4a-6a44-40d9-9295-898c27233946"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def query_vector_store_with_prompt(vectorstore_path, query_text, role=\"user\", top_k=3, detail_level=\"standard\"):\n",
        "    \"\"\"Query the vector store, retrieve relevant documents and generate a prompt template.\"\"\"\n",
        "    vectorstore = Chroma(persist_directory=vectorstore_path, embedding_function=OpenAIEmbeddings())\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
        "    results = retriever.get_relevant_documents(query_text)\n",
        "    context = \"\\n\".join([result.page_content for result in results])\n",
        "    prompt_template = generate_prompt_template(context, query_text, role, detail_level, references=results)\n",
        "\n",
        "    print(f\"Query: {query_text}\\n\")\n",
        "    for i, result in enumerate(results):\n",
        "        print(f\"Result {i + 1}:\")\n",
        "        print(f\"Content: {result.page_content}\\n\")\n",
        "        print(f\"Metadata: {result.metadata}\\n\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    # Return the generated prompt\n",
        "    return results, prompt_template\n",
        "\n",
        "\n",
        "# Example usage\n",
        "query_text = \"my tx expired, when I will get my funds back?\"\n",
        "vectorstore_path = \"vector_stores\"  # Update to your actual vector store path\n",
        "role = \"user\"  # Specify the role (\"developer\", \"admin\", \"user\")\n",
        "\n",
        "# Get the prompt template based on the query and role\n",
        "results, prompt = query_vector_store_with_prompt(vectorstore_path, query_text, role)\n",
        "\n",
        "\n",
        "import openai\n",
        "\n",
        "def generate_openai_response(prompt_template):\n",
        "    \"\"\"Generate a response from OpenAI based on the prompt.\"\"\"\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=prompt_template,\n",
        "            max_tokens=800,\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "        message = response.choices[0].message.content\n",
        "        return message\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating OpenAI response: {e}\")\n",
        "        return None\n",
        "\n",
        "# Print the generated prompt template\n",
        "print(json.dumps(prompt, indent=2))  # Pretty print the prompt for debugging purposes\n",
        "print(\"Response\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# Generate OpenAI response based on the prompt\n",
        "response = generate_openai_response(prompt)\n",
        "print(response)\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "# Format and print references with URLs and similarities below the response\n",
        "formatted_references = \"\\n\".join(\n",
        "    [f\"Source: {results[0].metadata.get('url')}\\n\"]\n",
        ")\n",
        "\n",
        "if formatted_references:\n",
        "    print(\"\\nReferences:\")\n",
        "    print(formatted_references)\n",
        "else:\n",
        "    print(\"\\nNo URL references found.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
